# -*- coding: utf-8 -*-
"""Task2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BgKCEsD7gNUUR7YEjG4iuSdHfGDkpuMl
"""



#Task 2 on dataset(D3) Predicting medical insurance costs

# Load Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Set display options to avoid output truncation
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 50)
pd.set_option('display.width', 1000)

# Step 1: Loading Data, Data Pre-processing, EDA

# Load Dataset
df = pd.read_csv('/content/drive/MyDrive/HIT391-Tutorial/insurance.csv')

# Data Preprocessing
# Encode categorical variables
le = LabelEncoder()
for col in ['sex', 'smoker', 'region']:
    df[col] = le.fit_transform(df[col])

# Check for missing values
print(df.isnull().sum())  # Should return 0 if data is clean

# Exploratory Data Analysis (EDA)
print(df.describe())
plt.figure(figsize=(8,6))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

# Scatter plot of charges vs age
plt.figure(figsize=(6,4))
sns.scatterplot(x='age', y='charges', data=df)
plt.title('Age vs Charges')
plt.show()

# Step 2: Feature Engineering, Creating Train, and Test Datasets
# Feature Engineering
X = df.drop('charges', axis=1)  # Independent Variables
y = df['charges']              # Target Variable

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Create Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Step 3: Apply 2 Regression Algorithms (Training and Testing)
# Algorithm 1: Linear Regression
lr = LinearRegression()
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)

# Algorithm 2: Random Forest Regressor
rf = RandomForestRegressor(random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

# Step 4: Generate 2 Evaluation Metrics on each algorithm

# Linear Regression Metrics
print("=== Linear Regression ===")
print("R2 Score:", r2_score(y_test, y_pred_lr))
print("RMSE:", np.sqrt(mean_squared_error(y_test, y_pred_lr)))

# Random Forest Metrics
print("\n=== Random Forest Regressor ===")
print("R2 Score:", r2_score(y_test, y_pred_rf))
print("RMSE:", np.sqrt(mean_squared_error(y_test, y_pred_rf)))

# Step 5: Compare Results
results = pd.DataFrame({
    'Model': ['Linear Regression', 'Random Forest'],
    'R2 Score': [r2_score(y_test, y_pred_lr), r2_score(y_test, y_pred_rf)],
    'RMSE': [np.sqrt(mean_squared_error(y_test, y_pred_lr)), np.sqrt(mean_squared_error(y_test, y_pred_rf))]
})
print("\n=== Comparison Table ===\n", results)

# Step 6: Fine Tune Best Algorithm (Random Forest)
from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [5, 10, 20, None],
    'min_samples_split': [2, 5, 10]
}

grid_search = GridSearchCV(RandomForestRegressor(random_state=42),
                           param_grid, cv=3, scoring='r2', n_jobs=-1)

grid_search.fit(X_train, y_train)

print("\nBest Parameters for Random Forest:", grid_search.best_params_)

# Final Evaluation with Tuned Model
best_rf = grid_search.best_estimator_
final_preds = best_rf.predict(X_test)
print("Final Tuned Random Forest R2 Score:", r2_score(y_test, final_preds))
print("Final Tuned Random Forest RMSE:", np.sqrt(mean_squared_error(y_test, final_preds)))